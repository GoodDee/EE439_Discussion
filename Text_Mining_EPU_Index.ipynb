{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining on Newspaper Articles Using NLTK  \n",
    "### EE439 Discussion Session 27 Feb 2021  \n",
    "#### Author: Kantapong Visantavarakul (https://github.com/GoodDee?tab=repositories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "1. Understand how text mining and data scraping work.  \n",
    "2. Understand the logic behind constructing Economic Policy Index.  \n",
    "3. References:  \n",
    "https://www.youtube.com/watch?v=YzMA2O_v5co&ab_channel=ComputerScience  \n",
    "http://www.nltk.org/book/ch01.html  \n",
    "https://www.kaggle.com/stieranka/text-analysis-operations-using-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src = \"./Activity_Page.png\", width = 800 />\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<img src = \"./Activity_Page.png\", width = 800 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download newspaper3k and nltk packages if you have not done so.\n",
    "#pip install newspaper3k\n",
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1: Newspaper Scraping from the Web  \n",
    "The objective is to fetch the newspaper from the website without copying and pasting the text into the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from newspaper import Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two article instances using Article from newspaper package\n",
    "URL1 = \"\"\n",
    "URL2 = \"\"\n",
    "article1 = Article(URL1)\n",
    "article2 = Article(URL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download two articles\n",
    "article1.download()\n",
    "article2.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse these two articles\n",
    "article1.parse()\n",
    "article2.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download punkt if you have not done so. This is used for tokenizing.\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sentence tokenization on these two articles\n",
    "article1.nlp()\n",
    "article2.nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The author of first article is {}\".format(article1.authors[0]))\n",
    "print(\"The author of first article is {}\".format(article2.authors[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what is inside this article\n",
    "print(article1.text)\n",
    "print(article2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can output the summary if you want to\n",
    "print(article1.summary)\n",
    "print(article2.summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guess what type of article2.text is? (A. string B. List)\n",
    "type(article2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2: Tokenize the text that we scraped into words that can be used later  \n",
    "The objective is to split the article into words such that we can further process and compute Economic Policy Uncertainty (EPU) Index in the final step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into sentences\n",
    "tokenized_text1=sent_tokenize(article1.text)\n",
    "tokenized_text2=sent_tokenize(article2.text)\n",
    "print(tokenized_text2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words (that is what we want!) and remove punctuations from consideration + make them all small letters\n",
    "tokenized_word1=word_tokenize(article1.text)\n",
    "words_1 =[word.lower() for word in tokenized_word1 if word.isalpha()]\n",
    "\n",
    "tokenized_word2=word_tokenize(article2.text)\n",
    "words_2 =[word.lower() for word in tokenized_word2 if word.isalpha()]\n",
    "print(words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords if you have not done so\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function to remove all stop words\n",
    "def RemoveStopWords(tokenized_words):\n",
    "    NoStopList = []\n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words:\n",
    "            NoStopList.append(word)\n",
    "    return NoStopList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1 = RemoveStopWords(words_1)\n",
    "words_2 = RemoveStopWords(words_2)\n",
    "\n",
    "print(words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming is reducing various word forms into a single one, using the word root.\n",
    "# For example, inflation, inflationary -> inflat\n",
    "\n",
    "def Stemmer(tokenized_words):\n",
    "    ps = PorterStemmer()\n",
    "    StemmedList = []\n",
    "    for word in tokenized_words:\n",
    "        StemmedList.append(ps.stem(word))\n",
    "    return StemmedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_1_stemmed = Stemmer(words_1)\n",
    "words_2_stemmed = Stemmer(words_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are other forms that did better than stemming such as Lemmatization that converts better to good. This requires dictionary look-up, and part-of-speech tagging before proceeding. This leads to the classic trade-off: accuracy vs performance (like what I discussed last week).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 3: Generate a toy example on Economic Policy Uncertainty Index (EPU)  \n",
    "The purpose of this final step is to generate true or false label on the news article. True label indicates that the article is related to policy. In index calculation, the higher it is (after normalization), the higher uncertainty level the journalists (public) perceived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P-tags are words related to policy. This directly captures policy-related variables. \n",
    "# E-tags are words about economics.\n",
    "# U-tags are words about uncertainties.\n",
    "P_Tags = []\n",
    "E_Tags = []\n",
    "U_Tags = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckEPU(word_list, P_tags, E_tags, U_tags):\n",
    "    \n",
    "    P_tags = Stemmer(P_tags)\n",
    "    E_tags = Stemmer(E_tags)\n",
    "    U_tags = Stemmer(U_tags)\n",
    "    \n",
    "    P_checked, E_checked, U_checked = False, False, False\n",
    "    \n",
    "    # For P, E and U tags, we require that the article contains one of these key terms\n",
    "    for word in word_list:\n",
    "        if word in P_tags:\n",
    "            P_checked = True\n",
    "            break\n",
    "            \n",
    "    for word in word_list:\n",
    "        if word in E_tags:\n",
    "            E_checked = True\n",
    "            break\n",
    "            \n",
    "    for word in word_list:\n",
    "        if word in U_tags:\n",
    "            U_checked = True\n",
    "            break\n",
    "    \n",
    "    # If the article contains words in P, E and U tags, then return True\n",
    "    if P_checked and E_checked and U_checked:\n",
    "        return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Article1_EPU = CheckEPU(words_1_stemmed)\n",
    "Article2_EPU = CheckEPU(words_2_stemmed)\n",
    "\n",
    "print(\"Economic Policy Uncertainty buckets on Article 1: {}\".format(str(Article1_EPU)))\n",
    "print(\"Economic Policy Uncertainty buckets on Article 2: {}\".format(str(Article2_EPU)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook illustrates how Economic Policy Uncertainty is detected on the news article in a toy setting. In reality, this requires much more effort in audit study, pilot study and selecting P terms in order to generate an accurate index.**    \n",
    "  \n",
    "**Inspiration from**: Baker, S.R., Bloom, N. and Davis, S.J., 2016. Measuring economic policy uncertainty. *The quarterly journal of economics, 131*(4), pp.1593-1636."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
